# app/tasks/generate_frequency.py

import logging
from datetime import datetime

from app.utils.date import get_today_kst
from app.utils.dynamo import (
    save_frequency_summary,
    get_frequency_by_category_and_date,
    get_news_by_category_and_date,
    update_news_card_content,
)
from app.services.openai_service import summarize_articles, cluster_similar_texts, summarize_group
from app.services.tts_service import text_to_speech
from app.utils.s3 import upload_audio_to_s3_presigned
from app.constants.category_map import CATEGORY_MAP
from app.services.deepsearch_service import extract_content_flexibly

# ë¡œê·¸ ì„¤ì •
logger = logging.getLogger()
logger.setLevel(logging.INFO)

def generate_all_frequencies():
    """
    âœ… ë§¤ì¼ ì˜¤ì „ 6ì‹œ ìë™ ì‹¤í–‰: ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ë³¸ë¬¸ ê¸°ë°˜ ê³µìœ  ëŒ€ë³¸/ìŒì„± ìƒì„±
    - ë‰´ìŠ¤ì¹´ë“œ DBì—ì„œ ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ì •í™•íˆ 30ê°œ ì‚¬ìš© (í† í° ìµœì í™”)
    - ë¶€ì¡±í•œ ë³¸ë¬¸ì€ ì¬ì¶”ì¶œ
    - GPT ìš”ì•½í•˜ì—¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (í´ëŸ¬ìŠ¤í„°ë§ í¬í•¨)
    - ElevenLabs TTSë¡œ ë³€í™˜ í›„ S3 ì—…ë¡œë“œ
    - Frequencies í…Œì´ë¸”ì— ì €ì¥ (ìŠ¤í¬ë¦½íŠ¸ + Presigned MP3 ë§í¬)
    """
    date = get_today_kst()
    all_categories = CATEGORY_MAP.keys()
    logger.info(f"ğŸŒ€ ì „ì²´ ì¹´í…Œê³ ë¦¬ ìˆœíšŒ ({len(all_categories)}ê°œ): {list(all_categories)}")

    for category_ko in all_categories:
        category_en = CATEGORY_MAP[category_ko]["api_name"]
        freq_id = f"{category_en}#{date}"

        # ì¤‘ë³µ ë°©ì§€: ì´ë¯¸ ìƒì„±ëœ ê²½ìš° ìŠ¤í‚µ
        if get_frequency_by_category_and_date(category_en, date):
            logger.info(f"ğŸš« ì´ë¯¸ ìƒì„±ë¨ â†’ ìŠ¤í‚µ: {freq_id}")
            continue

        logger.info(f"ğŸ“š ëŒ€ë³¸/ìŒì„± ìƒì„± ì‹œì‘: {category_en} ({freq_id})")

        try:
            # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ì˜¤ëŠ˜ ê¸°ì‚¬ ë¶ˆëŸ¬ì˜¤ê¸°
            articles = get_news_by_category_and_date(category_en, date)
            logger.info(f"ğŸ“¥ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(articles)}")

            full_contents = []
            processed_count = 0
            target_count = 30  # ì •í™•íˆ 30ê°œë¡œ ì œí•œ

            # ê¸°ì‚¬ ë³¸ë¬¸ ì •í™•íˆ 30ê°œê¹Œì§€ ìˆ˜ì§‘
            for i, article in enumerate(articles):
                if len(full_contents) >= target_count:
                    logger.info(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {target_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                    break
                    
                processed_count += 1
                news_id = article.get("news_id")
                url = article.get("content_url")
                content = article.get("content", "").strip()

                if not news_id or not url:
                    logger.warning(f"âš ï¸ #{processed_count} URL ë˜ëŠ” ID ì—†ìŒ â†’ ìŠ¤í‚µ")
                    continue

                # ë³¸ë¬¸ì´ ì§§ê±°ë‚˜ ì—†ìœ¼ë©´ ì¬ì¶”ì¶œ ì‹œë„
                if not content or len(content) < 300:
                    try:
                        content = extract_content_flexibly(url)
                        if content and len(content) >= 300:
                            update_news_card_content(news_id, content)
                            logger.info(f"â™»ï¸ #{processed_count} ë³¸ë¬¸ ë³´ì™„ ì €ì¥ ì™„ë£Œ ({len(content)}ì)")
                        else:
                            logger.warning(f"âš ï¸ #{processed_count} ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë„ˆë¬´ ì§§ìŒ â†’ ìŠ¤í‚µ")
                            continue
                    except Exception as e:
                        logger.warning(f"âŒ #{processed_count} ë³¸ë¬¸ ì¬ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue

                # ğŸ”§ í† í° ìµœì í™”: 3000ì â†’ 1500ìë¡œ ë‹¨ì¶•
                trimmed = content[:1500]
                full_contents.append(trimmed)
                logger.info(f"âœ… #{len(full_contents)} ë³¸ë¬¸ ì‚¬ìš© ì™„ë£Œ ({len(trimmed)}ì)")

            logger.info(f"ğŸ“Š ìµœì¢… ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(full_contents)}ê°œ (ëª©í‘œ: {target_count}ê°œ)")

            # ë³¸ë¬¸ ìˆ˜ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ
            if len(full_contents) < 5:
                logger.warning(f"âš ï¸ ìœ íš¨ ë³¸ë¬¸ ë¶€ì¡± ({len(full_contents)}ê°œ) â†’ ìŠ¤í‚µ: {category_en}")
                continue

            # â­ï¸ [1ì°¨ í´ëŸ¬ìŠ¤í„°ë§] ì›ë³¸ ê¸°ì‚¬ ë³¸ë¬¸ ê¸°ë°˜ ë¬¼ë¦¬ì  ì¤‘ë³µ ì œê±°
            logger.info(f"ğŸ”„ 1ì°¨ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘: {len(full_contents)}ê°œ ê¸°ì‚¬")
            try:
                groups = cluster_similar_texts(full_contents, threshold=0.80)
                group_summaries = []
                
                for group_idx, group in enumerate(groups):
                    if len(group) == 1:
                        # ë‹¨ì¼ ê¸°ì‚¬ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        group_summaries.append(group[0])
                        logger.info(f"ğŸ“„ ê·¸ë£¹ #{group_idx+1}: ë‹¨ì¼ ê¸°ì‚¬ ({len(group[0])}ì)")
                    else:
                        # ì—¬ëŸ¬ ìœ ì‚¬ ê¸°ì‚¬ â†’ ëŒ€í‘œ ìš”ì•½ë¬¸ ìƒì„±
                        try:
                            summary = summarize_group(group, category_en)
                            group_summaries.append(summary)
                            logger.info(f"ğŸ“Š ê·¸ë£¹ #{group_idx+1}: {len(group)}ê°œ ê¸°ì‚¬ â†’ í†µí•© ìš”ì•½ ({len(summary)}ì)")
                        except Exception as e:
                            logger.warning(f"âš ï¸ ê·¸ë£¹ #{group_idx+1} ìš”ì•½ ì‹¤íŒ¨, ì²« ë²ˆì§¸ ê¸°ì‚¬ ì‚¬ìš©: {e}")
                            group_summaries.append(group[0])
                
                logger.info(f"âœ… 1ì°¨ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(full_contents)}ê°œ â†’ {len(group_summaries)}ê°œ ê·¸ë£¹")
                final_contents = group_summaries
                
            except Exception as e:
                logger.warning(f"âš ï¸ 1ì°¨ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨, ì›ë³¸ ê¸°ì‚¬ ì‚¬ìš©: {e}")
                final_contents = full_contents

            # GPTë¡œ ì¢…í•© ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (2ì°¨ í´ëŸ¬ìŠ¤í„°ë§ í¬í•¨)
            logger.info(f"ğŸ“ ëŒ€ë³¸ ìƒì„± ì‹œì‘: {len(final_contents)}ê°œ ê¸°ì‚¬")
            script = summarize_articles(final_contents, category_en)
            if not script or len(script) < 500:
                logger.warning(f"âš ï¸ ìš”ì•½ ê¸¸ì´ ë¶€ì¡± â†’ ìŠ¤í‚µ: {category_en}")
                continue

            logger.info(f"ğŸ“ ìš”ì•½ ì™„ë£Œ: {len(script)}ì")

            # ElevenLabsë¡œ TTS ë³€í™˜ â†’ S3 Presigned URL ìƒì„±
            try:
                audio_bytes = text_to_speech(script)
                audio_url = upload_audio_to_s3_presigned(
                    file_bytes=audio_bytes,
                    user_id="shared",
                    category=category_en,
                    date=date,
                    expires_in_seconds=604800  # ğŸ”§ Presigned URL 7ì¼ ìœ íš¨ (24ì‹œê°„ â†’ 7ì¼)
                )
                logger.info(f"ğŸ”Š TTS Presigned URL ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"[TTS ì—…ë¡œë“œ ì‹¤íŒ¨] {category_en}: {str(e)}")
                continue

            # ê²°ê³¼ DynamoDBì— ì €ì¥
            item = {
                "frequency_id": freq_id,
                "category": category_en,
                "date": date,
                "script": script,
                "audio_url": audio_url,
                "created_at": datetime.utcnow().isoformat()
            }

            save_frequency_summary(item)
            logger.info(f"âœ… DynamoDB ì €ì¥ ì™„ë£Œ: {category_en} ({freq_id})")

        except Exception as e:
            logger.exception(f"[âŒ {category_en} ì²˜ë¦¬ ì‹¤íŒ¨] {str(e)}")